{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction and Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install the necessary packages\n",
    "\n",
    "You need the latest versions of these packages:                                                                                                                                     \n",
    "Watson Developer Cloud: a client library for Watson services.                                                                                                                        \n",
    "NLTK: leading platform for building Python programs to work with human language data.                                                                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Watson Developer Cloud package: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting watson-developer-cloud\n",
      "  Downloading https://files.pythonhosted.org/packages/41/35/9c98ba1056163641c97f1416e882679c2da941abb95d37311b90980c5293/watson-developer-cloud-1.3.5.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 2.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: requests<3.0,>=2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: python_dateutil>=2.5.3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Collecting autobahn>=0.10.9 (from watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/0c/af6e79f0cc23668454f4a73ec10b96bdb5b7f172509179da4fc92bce4142/autobahn-18.5.2-py2.py3-none-any.whl (299kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 2.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting Twisted>=13.2.0 (from watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/2a/e9e4fb2e6b2f7a75577e0614926819a472934b0b85f205ba5d5d2add54d0/Twisted-18.4.0.tar.bz2 (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 338kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: pyOpenSSL>=16.2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Collecting service-identity>=17.0.0 (from watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/fa/995e364220979e577e7ca232440961db0bf996b6edaf586a7d1bd14d81f1/service_identity-17.0.0-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python_dateutil>=2.5.3->watson-developer-cloud)\n",
      "Collecting txaio>=2.10.0 (from autobahn>=0.10.9->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/2e/c8a877b0a5c2798fa93ebcc1465a72a68c089e5f8b0a852ca335751dcc5a/txaio-2.10.0-py2.py3-none-any.whl\n",
      "Collecting zope.interface>=4.4.2 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/8a/657532df378c2cd2a1fe6b12be3b4097521570769d4852ec02c24bd3594e/zope.interface-4.5.0.tar.gz (151kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 6.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting constantly>=15.1 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
      "Collecting incremental>=16.10.1 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
      "Collecting Automat>=0.3.0 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/6a/1baf488c2015ecafda48c03ca984cf0c48c254622668eb1732dbe2eae118/Automat-0.6.0-py2.py3-none-any.whl\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/b6/84d0c863ff81e8e7de87cff3bd8fd8f1054c227ce09af1b679a8b17a9274/hyperlink-18.0.0-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: cryptography>=1.9 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Collecting pyasn1 (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/70/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 9.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/59/cedf87e91ed541be7957c501a92102f9cc6363c623a7666d69d51c78ac5b/attrs-18.1.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/51/bcd96bf6231d4b2cc5e023c511bee86637ba375c44a6f9d1b4b7ad1ce4b9/pyasn1_modules-0.2.1-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: setuptools in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from zope.interface>=4.4.2->Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: asn1crypto>=0.21.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: cffi>=1.7 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: pycparser in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cffi>=1.7->cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Building wheels for collected packages: watson-developer-cloud, Twisted, zope.interface\n",
      "  Running setup.py bdist_wheel for watson-developer-cloud ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/e4/21/d0/baf64c6565b076af23eb7e188bb5b55b12db4639bbc8aff27b\n",
      "  Running setup.py bdist_wheel for Twisted ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/b3/76/f7/85353c829c0881e74b5366ce0ed59042b098bb4903e2da8828\n",
      "  Running setup.py bdist_wheel for zope.interface ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/c6/b2/d2/be6785a207eaa58d76debc10c9d5c66196b40a88abb61d6af7\n",
      "Successfully built watson-developer-cloud Twisted zope.interface\n",
      "Installing collected packages: txaio, autobahn, zope.interface, constantly, incremental, attrs, Automat, hyperlink, Twisted, pyasn1, pyasn1-modules, service-identity, watson-developer-cloud\n",
      "Successfully installed Automat-0.6.0 Twisted-18.4.0 attrs-18.1.0 autobahn-18.5.2 constantly-15.1.0 hyperlink-18.0.0 incremental-17.5.0 pyasn1-0.4.3 pyasn1-modules-0.2.1 service-identity-17.0.0 txaio-2.10.0 watson-developer-cloud-1.3.5 zope.interface-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade watson-developer-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 725kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "Successfully installed nltk-3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install IBM Cloud Object Storage Client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement not upgraded as not directly required: ibm-cos-sdk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\n",
      "Requirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm-cos-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now restart the kernel by choosing Kernel > Restart. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import packages and libraries\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, KeywordsOptions\n",
    "    \n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import datetime\n",
    "from nltk import word_tokenize,sent_tokenize,ne_chunk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Add configurable items of the notebook below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Add your service credentials from IBM Cloud for the Watson services\n",
    "You must create a Watson Natural Language Understanding service on IBM Cloud. Create a service for Natural Language Understanding (NLU). Insert the username and password values for your NLU in the following cell. Do not change the values of the version fields.\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2018-03-23',\n",
    "    username=\"\",\n",
    "    password=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Add your service credentials for Object Storage\n",
    "You must create Object Storage service on IBM Cloud. To access data in a file in Object Storage, you need the Object Storage authentication credentials. Insert the Object Storage authentication credentials as credentials_1 in the following cell after removing the current contents in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials_1 = {\n",
    "    'IBM_API_KEY_ID': '',\n",
    "    'IAM_SERVICE_ID': '',\n",
    "    'ENDPOINT': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'BUCKET': '',\n",
    "    'FILE': 'form-doc-1.txt'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Global Variables\n",
    "Add global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText='form-doc-1.txt'\n",
    "ConfigFileName_Entity='config_entity_extract.txt'\n",
    "ConfigFileName_Classify= 'config_legaldocs.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Configure and download required NLTK packages\n",
    "Download the 'punkt' and 'averaged_perceptron_tagger' NLTK packages for POS tagging usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dsxuser/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Persistence and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Configure Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.client('s3',\n",
    "                    ibm_api_key_id=credentials_1['IBM_API_KEY_ID'],\n",
    "                    ibm_service_instance_id=credentials_1['IAM_SERVICE_ID'],\n",
    "                    ibm_auth_endpoint=credentials_1['IBM_AUTH_ENDPOINT'],\n",
    "                    config=Config(signature_version='oauth'),\n",
    "                    endpoint_url=credentials_1['ENDPOINT'])\n",
    "\n",
    "def get_file(filename):\n",
    "    '''Retrieve file from Cloud Object Storage'''\n",
    "    fileobject = cos.get_object(Bucket=credentials_1['BUCKET'], Key=filename)['Body']\n",
    "    return fileobject\n",
    "\n",
    "def load_string(fileobject):\n",
    "    '''Load the file contents into a Python string'''\n",
    "    text = fileobject.read()\n",
    "    return text\n",
    "\n",
    "def put_file(filename, filecontents):\n",
    "    '''Write file to Cloud Object Storage'''\n",
    "    resp = cos.put_object(Bucket=credentials_1['BUCKET'], Key=filename, Body=filecontents)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Input Data\n",
    "Read the data file for entity extraction from Object Store                                                                                                                               \n",
    "Read the configuration file for augumented entity-value pairs from Object Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿PURCHASE AGREEMENT\r\n",
      "\r\n",
      "THIS IS A LEGALLY BINDING CONTRACT BETWEEN\r\n",
      "PURCHASER AND SELLER.\r\n",
      "IF YOU DO NOT UNDERSTAND IT, SEEK LEGAL ADVICE.\r\n",
      "\r\n",
      "1. PARTIES TO CONTRACT - PROPERTY. Purchaser and Seller acknowledge that Broker is ABC\r\n",
      "is not the limited agent of both parties to this transaction as outlined in Section III of the Agency\r\n",
      "Agreement Addendum as authorized by Purchaser and Seller.\r\n",
      "\r\n",
      ", XYZ hereinafter referred to as\r\n",
      "\r\n",
      "Purchaser, offers and agrees to purchase from , UVW\r\n",
      "hereinafter referred to as Seller, upon the tenns and conditions set forth, the property legally described as:\r\n",
      "\r\n",
      "also known as\r\n",
      "\r\n",
      "2. EARNEST MONEY DEPOSIT. Earnest Money in the amount of ($ )\r\n",
      "DOLLARS Cash Check ,\r\n",
      "unless otherwise noted herein, shall be deposited into the trust account of the listing selling\r\n",
      "\r\n",
      "broker on the next legal banking day after acceptance of this offer.\r\n",
      "\r\n",
      "Other earnest money provisions:\r\n",
      "\r\n",
      "3. PURCHASE PRICE. The total purchase price is to be ($ )\r\n",
      "DOLLARS\r\n",
      "\r\n",
      "After earnest money herein is credited, the remaining balance is to be paid by Purchaser at closing.\r\n",
      "\r\n",
      "4. F INANCING.\r\n",
      "‘Jew Mortgage. This offer is contingent upon Purchaser obtaining a new\r\n",
      "\r\n",
      "VA, FHA, SDHDA, Conventional, or type of loan.\r\n",
      "A letter of Purchaser’s loan status from\r\n",
      "\r\n",
      "is attached or will be delivered by (date).\r\n",
      "\r\n",
      "Within T legal banking days after acceptance of this Agreement, Purchaser will make application\r\n",
      "for and diligently and in good faith endeavor to secure a new loan, pay all application fees, and to sign\r\n",
      "all financing documents without delay. Purchaser reserves the right to obtain alternative financing as\r\n",
      "long as there are no increased costs to Seller.\r\n",
      "\r\n",
      "Assumption. See attached Addendum.\r\n",
      "Contract for Deed/Private Mortgage. See attached Addendum.\r\n",
      "\r\n",
      "Cash. This is a cash offer. The remaining balance of “.6 will be paid at closing by\r\n",
      "certified check. A letter of veriﬁcation from\r\n",
      "\r\n",
      "regarding the availability of funds is attached will be delivered by (date) or\r\n",
      "this agreement, at the option of Seller without notice to Purchaser may be voided.\r\n",
      "\r\n",
      "INITIALS: PURCHASER / SELLER /\r\n",
      "\r\n",
      "Page 1 of 5\r\n",
      "SDREC.RESIDENTIALPURCHASEAGREEMENT.201 1\n"
     ]
    }
   ],
   "source": [
    "text_file= load_string(get_file(sampleText))\n",
    "if isinstance(text_file, bytes):\n",
    "    text_file = text_file.decode('utf-8') \n",
    "print(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "\t\"configuration\":{\r\n",
      "\t\t\"class\":{\r\n",
      "\r\n",
      "\t\t\t\t\"stages\" : [\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\"name\": \"Intro\",\r\n",
      "\t\t\t\t\t\"steps\": [\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"type\":\"text\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"Landlord\",\r\n",
      "\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <NNP> (<IN> <VBP>)?}\"\r\n",
      "\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"type\":\"text\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"Tenant\",\r\n",
      "\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <NNP> (<IN> <VBP>)?}\"\r\n",
      "\t\t\t\t\t},\r\n",
      "\t\t\t\t\t{\r\n",
      "\r\n",
      "\t\t\t\t\t\t\"type\":\"date\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"Date\",\r\n",
      "\t\t\t\t\t\t\"regex1\":\"\\\\d+/\\\\d+/\\\\d+\"\r\n",
      "\t\t\t\t\t}\r\n",
      "\r\n",
      "\t\t\t\t\t]\r\n",
      "\r\n",
      "\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\"name\": \"Term\",\r\n",
      "\t\t\t\t\t\"steps\":[\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"term_type\": \"Fixed\",\r\n",
      "\t\t\t\t\t\t\"type\":\"date\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"beginning on\",\r\n",
      "\t\t\t\t\t\t\"regex\":\"\\\\d+/\\\\d+/\\\\d+\"\r\n",
      "\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"term_type\": \"Fixed\",\r\n",
      "\t\t\t\t\t\t\"type\":\"date\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"ending on\",\r\n",
      "\t\t\t\t\t\t\"regex\":\"\\\\d+/\\\\d+/\\\\d+\"\r\n",
      "\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"term_type\": \"Month\",\r\n",
      "\t\t\t\t\t\t\"type\":\"date\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"beginning on\",\r\n",
      "\t\t\t\t\t\t\"regex\":\"\\\\d+/\\\\d+/\\\\d+\"\r\n",
      "\t\t\t\t\t}\r\n",
      "\t\t\t\t\t]\r\n",
      "\r\n",
      "\r\n",
      "\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\"name\": \"Rent\",\r\n",
      "\t\t\t\t\t\"steps\":[\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"type\":\"amount\",\r\n",
      "\t\t\t\t\t\t\"tag\": \"Rental amt\",\r\n",
      "\t\t\t\t\t\t\"regex\":\"\\\\$\\\\s+\\\\d+\"\r\n",
      "\t\t\t\t\t}\r\n",
      "\t\t\t\t\t]\r\n",
      "\r\n",
      "\r\n",
      "\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"name\": \"Parties to Contract\",\r\n",
      "\t\t\t\t\t\t\"steps\": [\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"type\":\"text\",\r\n",
      "\t\t\t\t\t\t\t\"tag\": \"Broker\",\r\n",
      "\t\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <VBZ> <NNP>}\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"type\":\"text\",\r\n",
      "\t\t\t\t\t\t\t\"tag\": \"Purchaser\",\r\n",
      "\t\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <NN> <VBD> <TO> <IN> <NNP>}\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"type\":\"text\",\r\n",
      "\t\t\t\t\t\t\t\"tag\": \"Seller\",\r\n",
      "\t\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <NN> <VBD> <TO> <IN> <NNP>}\"\r\n",
      "\t\t\t\t\t\t}\r\n",
      "\t\t\t\t\t\t]\r\n",
      "\r\n",
      "\t\t\t\t\t}\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\t\t\t\t]\r\n",
      "\r\n",
      "\r\n",
      "\t\t}\r\n",
      "\t}\r\n",
      "}\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_entity = load_string(get_file(ConfigFileName_Entity)).decode('utf-8')\n",
    "print(config_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "\t\"configuration\":{\r\n",
      "\t\t\"classification\":{\r\n",
      "\t\t\t\"stages\":[\r\n",
      "\t\t\t\t{\r\n",
      "\t\t\t\t\t\"doctype\":\"Rental\",\r\n",
      "\t\t\t\t\t\"entities\":[\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"Lease Term\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"lease term\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"Rent\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"Rent\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"Security Deposit\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"Security Deposit\"\r\n",
      "\t\t\t\t\t\t}\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t]\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t{\r\n",
      "\t\t\t\t\t\"doctype\":\"Purchase\",\r\n",
      "\t\t\t\t\t\"entities\":[\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"PARTIES TO CONTRACT - PROPERTY\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"PARTIES TO CONTRACT - PROPERTY\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"EARNEST MONEY DEPOSIT\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"EARNEST MONEY DEPOSIT\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"PURCHASE PRICE\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"PURCHASE PRICE\"\r\n",
      "\t\t\t\t\t\t}\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t]\r\n",
      "\t\t\t\t}\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t]\r\n",
      "\t\t\r\n",
      "\t\t}\r\n",
      "\t\r\n",
      "\t}\r\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config_class = load_string(get_file(ConfigFileName_Classify)).decode('utf-8')\n",
    "print(config_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entity Extraction\n",
    "Extract required entities present in the document and augment the response to NLU's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Entites Extracted by Watson NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_using_NLU(analysistext):\n",
    "    \"\"\" Call Watson Natural Language Understanding service to obtain analysis results.\n",
    "    \"\"\"\n",
    "    response = natural_language_understanding.analyze( \n",
    "        text=analysistext,\n",
    "        features=Features(keywords=KeywordsOptions()))\n",
    "    response = [r['text'] for r in response['keywords']]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Extract Entity-Value \n",
    "Custom entity extraction utlity fucntions for augumenting the results of Watson NLU API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    sent = re.sub(r'\\n',' ',text)\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    POSofText = nltk.tag.pos_tag(words)\n",
    "    return POSofText\n",
    "\n",
    "\n",
    "entval= dict()\n",
    "def text_extract(reg, tag,text):\n",
    "    \"\"\" Use Chunking to extract text from sentence\n",
    "    \"\"\"\n",
    "    entities = list()\n",
    "    chunkParser= nltk.RegexpParser(reg)\n",
    "    chunked= chunkParser.parse(POS_tagging(text))\n",
    "    #print(chunked)\n",
    "    for subtree in chunked.subtrees():\n",
    "        if subtree.label() == 'Chunk':\n",
    "            #print(subtree.leaves())\n",
    "            entities.append(subtree.leaves())\n",
    "    #print(entities)\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(len(entities[i])):\n",
    "            #print(entities[i][j][0].lower())\n",
    "            if tag.strip().lower() in entities[i][j][0].lower():\n",
    "                #print(entities[i])\n",
    "                entval.update({tag: find_NNP(entities[i],tag)})\n",
    "    return entval\n",
    "\n",
    "\n",
    "def find_NNP(ent, tag):\n",
    "    \"\"\" Find NNP POS tags\n",
    "    \"\"\"\n",
    "    e= ent\n",
    "    for i in range(len(e)):\n",
    "        if (tag not in e[i]) and (e[i][1] == 'NNP'):\n",
    "            return e[i][0]\n",
    "\n",
    "\n",
    "\n",
    "def checkValid(date):\n",
    "    #f= datetime.datetime.strftime(date)\n",
    "    try:\n",
    "        datetime.datetime.strptime(date.strip(),\"%d/%m/%Y\")\n",
    "        return 1\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        return 0\n",
    "    \n",
    "def date_extract(reg, tag, text, stage_name):\n",
    "    #print(reg)\n",
    "    d= dict()\n",
    "    dates=re.findall(tag.lower()+' '+reg,text.lower())\n",
    "    print(dates)\n",
    "    temp= dates[0].strip(tag.lower())\n",
    "    ret= checkValid(temp)\n",
    "    if ret == 1:\n",
    "        d.update({tag.lower():temp})\n",
    "    print(d)\n",
    "\n",
    "def amt_extract(reg,tag,text):\n",
    "    a= dict()\n",
    "    amt= re.findall(reg,text)\n",
    "    print(amt)\n",
    "    \n",
    "entities_req= list()\n",
    "def entities_required(text,step, types):\n",
    "    \"\"\" Extracting entities required from configuration file\n",
    "    \"\"\"\n",
    "    configjson= json.loads(config_entity)\n",
    "    for i in range(len(step)):\n",
    "        if step[i]['type'] == types:\n",
    "            entities_req.append(str(step[i]['tag']))\n",
    "            #entities_req.append([c['tag'] for c in configjson['configuration']['class'][i]['steps'][j]])\n",
    "    return entities_req\n",
    "\n",
    "# entlist= list()\n",
    "def extract_entities(config,text):\n",
    "    \"\"\" Extracts entity-value pairs\n",
    "    \"\"\"\n",
    "    configjson= json.loads(config)\n",
    "    #print(configjson)\n",
    "    #print(configjson['configuration']['class'][0]['steps'][0]['entity'][0]['tag'])\n",
    "    classes=configjson['configuration']['class']\n",
    "    #for i in range(len(classes)):\n",
    "    stages= classes['stages']\n",
    "    for j in range(len(stages)):\n",
    "        if stages[j]['name']=='Intro':\n",
    "            steps= stages[j]['steps']\n",
    "            for k in range(len(steps)):\n",
    "                if steps[k]['type'] == 'text':\n",
    "                        #temp=entities_required(text,steps,steps[k]['type'])\n",
    "                            #print(temp)\n",
    "                    ent = text_extract(steps[k]['regex'],steps[k]['tag'],text)\n",
    "                #elif steps[k]['type'] == 'date':\n",
    "                    #dates= date_extract(steps[k]['regex1'],steps[k]['tag'],text, stages[j]['name'])\n",
    "        elif stages[j]['name']=='Parties to Contract':\n",
    "            steps= stages[j]['steps']\n",
    "            for k in range(len(steps)):\n",
    "                if steps[k]['type'] == 'text':\n",
    "                        #temp=entities_required(text,steps,steps[k]['type'])\n",
    "                    ent = text_extract(steps[k]['regex'],steps[k]['tag'],text)\n",
    "                    #print(ent)\n",
    "    \n",
    "    return ent\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Broker': 'ABC', 'Purchaser': 'XYZ', 'Seller': 'UVW'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "extract_entities(config_entity, text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Classification\n",
    "Classify documents based on entities extracted from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_required_classification(text,config):\n",
    "    \"\"\" Extracting entities from configuration file\n",
    "    \"\"\"\n",
    "    entities_req= list()\n",
    "    configjson= json.loads(config)\n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        class_req= stages['doctype']\n",
    "        entities_req.append([[c['text'],class_req] for c in stages['entities']])\n",
    "    return entities_req\n",
    "#entities_required_classification(text2,config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, entities,config):\n",
    "    \"\"\" Classify type of document from list of entities(NLU + Configuration file)\n",
    "    \"\"\"\n",
    "    e= dict()\n",
    "    entities_req= entities_required_classification(text,config)\n",
    "    for i in range(len(entities_req)):\n",
    "        temp= list()\n",
    "        for j in range(len(entities_req[i])):\n",
    "            entities_req[i][j][0]= entities_req[i][j][0].strip()\n",
    "            entities_req[i][j][0]= entities_req[i][j][0].lower()\n",
    "            temp.append(entities_req[i][j][0])\n",
    "            res= analyze_using_NLU(text)\n",
    "            #temp= temp + res\n",
    "            #print text\n",
    "            #text= text.decode('utf-8')\n",
    "        if all(str(x) in text.lower() for x in temp) and any(str(y) in text.lower() for y in res):\n",
    "            return entities_req[i][j][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_classify(text,config,config1):\n",
    "    \"\"\" Classify type of Document\n",
    "    \"\"\"\n",
    "    entities= analyze_using_NLU(text)\n",
    "    temp= extract_entities(config,text)\n",
    "    for k,v in temp.items():\n",
    "        entities.append(k)\n",
    "    #print(entities)\n",
    "    entities= [e.lower() for e in entities]\n",
    "    entities= [e.strip() for e in entities]\n",
    "    entities= set(entities)\n",
    "    ret=classify_text(text,entities,config1)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Purchase'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_classify(text_file,config_entity,config_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
